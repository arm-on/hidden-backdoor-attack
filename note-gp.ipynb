{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "import sys\n",
    "import numpy as np\n",
    "import pdb\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import configparser\n",
    "\n",
    "from PIL import Image\n",
    "from alexnet_fc7out import alexnet, NormalizeByChannelMeanStd\n",
    "from dataset import PoisonGenerationDataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from torch import cuda\n",
    "from torch.backends import mps\n",
    "import glob\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else ('mps' if mps.is_available() else 'cpu')\n",
    "seed = 50\n",
    "experimentID = 'arman'\n",
    "patch_size = 30\n",
    "trigger_id = 10\n",
    "eps = 16\n",
    "logfile = 'logs/report.log'\n",
    "epochs = 2\n",
    "lr = 0.01\n",
    "num_source = 1\n",
    "rand_loc = True\n",
    "target_wnid = 'n02437312'\n",
    "num_iter = 2\n",
    "source_wnid_list = 'source_wnid_list.txt'\n",
    "data_root = '/Users/armanmalekzadeh/Documents/GitHub/hidden-trigger-backdoor-attack/data' # should contain a folder named `train` (containing n0123123 folders) and another folder named `test` with the same structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added by me\n",
    "with open(f'ImageNet_data_list/poison_generation/{target_wnid}.txt', 'w+') as file:\n",
    "    filelist = glob.glob(os.path.join(data_root, 'train', str(target_wnid), '*.JPEG'), recursive=True)\n",
    "    filelist = [f'{target_wnid}/' + os.path.basename(file_path) + ('\\n' if idx!=len(filelist)-1 else '') for idx, file_path in enumerate(filelist)]\n",
    "    file.writelines(filelist)\n",
    "    \n",
    "with open(source_wnid_list, 'r') as file:\n",
    "    all_source_wnids = file.readlines()\n",
    "all_source_wnids = [wnid.strip() for wnid in all_source_wnids]\n",
    "for wnid in all_source_wnids:\n",
    "    with open(f'ImageNet_data_list/poison_generation/{wnid}.txt', 'w+') as file:\n",
    "        filelist = glob.glob(os.path.join(data_root, 'train', str(wnid), '*.JPEG'), recursive=True)\n",
    "        filelist = [f'{wnid}/' + os.path.basename(file_path) + ('\\n' if idx!=len(filelist)-1 else '') for idx, file_path in enumerate(filelist)]\n",
    "        file.writelines(filelist)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveDir_poison = \"poison_data/\" + experimentID + \"/rand_loc_\" +  str(rand_loc) + '/eps_' + str(eps) + \\\n",
    "\t\t\t\t\t'/patch_size_' + str(patch_size) + '/trigger_' + str(trigger_id)\n",
    "saveDir_patched = \"patched_data/\" + experimentID + \"/rand_loc_\" +  str(rand_loc) + '/eps_' + str(eps) + \\\n",
    "\t\t\t\t\t'/patch_size_' + str(patch_size) + '/trigger_' + str(trigger_id)\n",
    "\n",
    "if os.path.exists(saveDir_poison):\n",
    "    shutil.rmtree(saveDir_poison)\n",
    "if os.path.exists(saveDir_patched):\n",
    "    shutil.rmtree(saveDir_patched)\n",
    "\n",
    "if not os.path.exists(saveDir_poison):\n",
    "\tos.makedirs(saveDir_poison)\n",
    "if not os.path.exists(saveDir_patched):\n",
    "\tos.makedirs(saveDir_patched)\n",
    "\n",
    "if not os.path.exists(\"data/{}\".format(experimentID)):\n",
    "\tos.makedirs(\"data/{}\".format(experimentID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(img):\n",
    "\tnpimg = img.numpy()\n",
    "\t# plt.figure()\n",
    "\tplt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(img, fname):\n",
    "\timg = img.data.numpy()\n",
    "\timg = np.transpose(img, (1, 2, 0))\n",
    "\timg = img[: , :, ::-1]\n",
    "\tcv2.imwrite(fname, np.uint8(255 * img), [cv2.IMWRITE_PNG_COMPRESSION, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def adjust_learning_rate(lr, iter):\n",
    "\t\"\"\"Sets the learning rate to the initial LR decayed by 0.5 every 1000 iterations\"\"\"\n",
    "\tlr = lr * (0.5 ** (iter // 1000))\n",
    "\treturn lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "\t\"\"\"Computes and stores the average and current value\"\"\"\n",
    "\tdef __init__(self):\n",
    "\t\tself.reset()\n",
    "\n",
    "\tdef reset(self):\n",
    "\t\tself.val = 0\n",
    "\t\tself.avg = 0\n",
    "\t\tself.sum = 0\n",
    "\t\tself.count = 0\n",
    "\n",
    "\tdef update(self, val, n=1):\n",
    "\t\tself.val = val\n",
    "\t\tself.sum += val * n\n",
    "\t\tself.count += n\n",
    "\t\tself.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch):\n",
    "\n",
    "\tsince = time.time()\n",
    "\t# AVERAGE METER\n",
    "\tlosses = AverageMeter()\n",
    "\n",
    "\t# TRIGGER PARAMETERS\n",
    "\ttrans_image = transforms.Compose([transforms.Resize((224, 224)),\n",
    "\t\t\t\t\t\t\t\t\t  transforms.ToTensor(),\n",
    "\t\t\t\t\t\t\t\t\t  ])\n",
    "\ttrans_trigger = transforms.Compose([transforms.Resize((patch_size, patch_size)),\n",
    "\t\t\t\t\t\t\t\t\t\ttransforms.ToTensor(),\n",
    "\t\t\t\t\t\t\t\t\t\t])\n",
    "\n",
    "\t# PERTURBATION PARAMETERS\n",
    "\teps1 = (eps/255.0)\n",
    "\tlr1 = lr\n",
    "\n",
    "\ttrigger = Image.open('data/triggers/trigger_{}.png'.format(trigger_id)).convert('RGB')\n",
    "\ttrigger = trans_trigger(trigger).unsqueeze(0).to(device)\n",
    "\n",
    "\t# SOURCE AND TARGET DATASETS\n",
    "\ttarget_filelist = \"ImageNet_data_list/poison_generation/\" + target_wnid + \".txt\"\n",
    "\n",
    "\t# Use source wnid list\n",
    "\tif num_source==1:\n",
    "\t\tlogging.info(\"Using single source for this experiment.\")\n",
    "\telse:\n",
    "\t\tlogging.info(\"Using multiple source for this experiment.\")\n",
    "\n",
    "\twith open(\"data/{}/multi_source_filelist.txt\".format(experimentID),\"w\") as f1:\n",
    "\t\twith open(source_wnid_list) as f2:\n",
    "\t\t\tsource_wnids = f2.readlines()\n",
    "\t\t\tsource_wnids = [s.strip() for s in source_wnids]\n",
    "\n",
    "\t\t\tfor source_wnid in source_wnids:\n",
    "\t\t\t\twith open(\"ImageNet_data_list/poison_generation/\" + source_wnid + \".txt\", \"r\") as f2:\n",
    "\t\t\t\t\tshutil.copyfileobj(f2, f1)\n",
    "\n",
    "\tsource_filelist = \"data/{}/multi_source_filelist.txt\".format(experimentID)\n",
    "\n",
    "\n",
    "\tdataset_target = PoisonGenerationDataset(data_root + \"/train\", target_filelist, trans_image)\n",
    "\tdataset_source = PoisonGenerationDataset(data_root + \"/train\", source_filelist, trans_image)\n",
    "\n",
    "\t# SOURCE AND TARGET DATALOADERS\n",
    "\ttrain_loader_target = torch.utils.data.DataLoader(dataset_target,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tbatch_size=100,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tshuffle=True,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tnum_workers=8,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tpin_memory=True)\n",
    "\n",
    "\ttrain_loader_source = torch.utils.data.DataLoader(dataset_source,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t  batch_size=100,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t  shuffle=True,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t  num_workers=8,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t  pin_memory=True)\n",
    "\n",
    "\n",
    "\tlogging.info(\"Number of target images:{}\".format(len(dataset_target)))\n",
    "\tlogging.info(\"Number of source images:{}\".format(len(dataset_source)))\n",
    "\n",
    "\t# USE ITERATORS ON DATALOADERS TO HAVE DISTINCT PAIRING EACH TIME\n",
    "\titer_target = iter(train_loader_target)\n",
    "\titer_source = iter(train_loader_source)\n",
    "\n",
    "\tnum_poisoned = 0\n",
    "\tfor i in range(len(train_loader_target)):\n",
    "\n",
    "\t\t# LOAD ONE BATCH OF SOURCE AND ONE BATCH OF TARGET\n",
    "\t\t(input1, path1) = next(iter_source)\n",
    "\t\t(input2, path2) = next(iter_target)\n",
    "\n",
    "\t\timg_ctr = 0\n",
    "\n",
    "\t\tinput1 = input1.to(device)\n",
    "\t\tinput2 = input2.to(device)\n",
    "\t\tpert = nn.Parameter(torch.zeros_like(input2, requires_grad=True).to(device))\n",
    "\n",
    "\t\tfor z in range(input1.size(0)):\n",
    "\t\t\tif not rand_loc:\n",
    "\t\t\t\tstart_x = 224-patch_size-5\n",
    "\t\t\t\tstart_y = 224-patch_size-5\n",
    "\t\t\telse:\n",
    "\t\t\t\tstart_x = random.randint(0, 224-patch_size-1)\n",
    "\t\t\t\tstart_y = random.randint(0, 224-patch_size-1)\n",
    "\n",
    "\t\t\t# PASTE TRIGGER ON SOURCE IMAGES\n",
    "\t\t\tinput1[z, :, start_y:start_y+patch_size, start_x:start_x+patch_size] = trigger\n",
    "\n",
    "\t\toutput1, feat1 = model(input1)\n",
    "\t\tfeat1 = feat1.detach().clone()\n",
    "\n",
    "\t\tfor k in range(input1.size(0)):\n",
    "\t\t\timg_ctr = img_ctr+1\n",
    "\t\t\t# input2_pert = (pert[k].clone().cpu())\n",
    "\n",
    "\t\t\tfname = saveDir_patched + '/' + 'badnet_' + str(os.path.basename(path1[k])).split('.')[0] + '_' + 'epoch_' + str(epoch).zfill(2)\\\n",
    "\t\t\t\t\t+ str(img_ctr).zfill(5)+'.png'\n",
    "\n",
    "\t\t\tsave_image(input1[k].clone().cpu(), fname)\n",
    "\t\t\tnum_poisoned +=1\n",
    "\n",
    "\t\tfor j in range(num_iter):\n",
    "\t\t\tlr1 = adjust_learning_rate(lr, j)\n",
    "\n",
    "\t\t\toutput2, feat2 = model(input2+pert)\n",
    "\n",
    "\t\t\t# FIND CLOSEST PAIR WITHOUT REPLACEMENT\n",
    "\t\t\tfeat11 = feat1.clone()\n",
    "\t\t\tdist = torch.cdist(feat1, feat2)\n",
    "\t\t\tfor _ in range(feat2.size(0)):\n",
    "\t\t\t\tdist_min_index = (dist == torch.min(dist)).nonzero().squeeze()\n",
    "\t\t\t\tfeat1[dist_min_index[1]] = feat11[dist_min_index[0]]\n",
    "\t\t\t\tdist[dist_min_index[0], dist_min_index[1]] = 1e5\n",
    "\n",
    "\t\t\tloss1 = ((feat1-feat2)**2).sum(dim=1)\n",
    "\t\t\tloss = loss1.sum()\n",
    "\n",
    "\t\t\tlosses.update(loss.item(), input1.size(0))\n",
    "\n",
    "\t\t\tloss.backward()\n",
    "\n",
    "\t\t\tpert = pert- lr1*pert.grad\n",
    "\t\t\tpert = torch.clamp(pert, -eps1, eps1).detach_()\n",
    "\n",
    "\t\t\tpert = pert + input2\n",
    "\n",
    "\t\t\tpert = pert.clamp(0, 1)\n",
    "\n",
    "\t\t\tif j%100 == 0:\n",
    "\t\t\t\tlogging.info(\"Epoch: {:2d} | i: {} | iter: {:5d} | LR: {:2.4f} | Loss Val: {:5.3f} | Loss Avg: {:5.3f}\"\n",
    "\t\t\t\t\t\t\t .format(epoch, i, j, lr1, losses.val, losses.avg))\n",
    "\n",
    "\t\t\tif loss1.max().item() < 10 or j == (num_iter-1):\n",
    "\t\t\t\tfor k in range(input2.size(0)):\n",
    "\t\t\t\t\timg_ctr = img_ctr+1\n",
    "\t\t\t\t\tinput2_pert = (pert[k].clone().cpu())\n",
    "\n",
    "\t\t\t\t\tfname = saveDir_poison + '/' + 'loss_' + str(int(loss1[k].item())).zfill(5) + '_' + 'epoch_' + \\\n",
    "\t\t\t\t\t\t\tstr(epoch).zfill(2) + '_' + str(os.path.basename(path2[k])).split('.')[0] + '_' + \\\n",
    "\t\t\t\t\t\t\tstr(os.path.basename(path1[k])).split('.')[0] + '_kk_' + str(img_ctr).zfill(5)+'.png'\n",
    "\n",
    "\t\t\t\t\tsave_image(input2_pert, fname)\n",
    "\t\t\t\t\tnum_poisoned +=1\n",
    "\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\tpert = pert - input2\n",
    "\t\t\tpert.requires_grad = True\n",
    "\n",
    "\ttime_elapsed = time.time() - since\n",
    "\tlogging.info('Training complete one epoch in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-03 13:08:43,168 Experiment ID: arman\n",
      "/var/folders/2k/w49p8c6n2s14x29kyn06m2pr0000gn/T/ipykernel_19514/1449311122.py:18: UserWarning: You have chosen to seed training. This will turn on the CUDNN deterministic setting, which can slow down your training considerably! You may see unexpected behavior when restarting from checkpoints.\n",
      "  warnings.warn('You have chosen to seed training. '\n",
      "2024-03-03 13:08:43,196 Use GPU: mps for training\n",
      "2024-03-03 13:08:43,199 => using pre-trained model 'alexnet'\n",
      "2024-03-03 13:08:46,120 Using single source for this experiment.\n",
      "2024-03-03 13:08:46,158 Number of target images:1300\n",
      "2024-03-03 13:08:46,158 Number of source images:1300\n",
      "2024-03-03 13:09:29,504 Epoch:  0 | i: 0 | iter:     0 | LR: 0.0100 | Loss Val: 1092784.250 | Loss Avg: 1092784.250\n",
      "2024-03-03 13:09:34,803 Epoch:  0 | i: 1 | iter:     0 | LR: 0.0100 | Loss Val: 1061889.875 | Loss Avg: 968805.188\n",
      "2024-03-03 13:09:39,567 Epoch:  0 | i: 2 | iter:     0 | LR: 0.0100 | Loss Val: 1127817.250 | Loss Avg: 954077.125\n",
      "2024-03-03 13:09:44,195 Epoch:  0 | i: 3 | iter:     0 | LR: 0.0100 | Loss Val: 1117077.500 | Loss Avg: 953381.562\n",
      "2024-03-03 13:09:49,020 Epoch:  0 | i: 4 | iter:     0 | LR: 0.0100 | Loss Val: 1137324.500 | Loss Avg: 955758.861\n",
      "2024-03-03 13:09:53,636 Epoch:  0 | i: 5 | iter:     0 | LR: 0.0100 | Loss Val: 1119228.000 | Loss Avg: 953811.364\n",
      "2024-03-03 13:09:59,528 Epoch:  0 | i: 6 | iter:     0 | LR: 0.0100 | Loss Val: 1179296.750 | Loss Avg: 958664.630\n",
      "2024-03-03 13:10:04,448 Epoch:  0 | i: 7 | iter:     0 | LR: 0.0100 | Loss Val: 1115978.750 | Loss Avg: 958041.221\n",
      "2024-03-03 13:10:09,234 Epoch:  0 | i: 8 | iter:     0 | LR: 0.0100 | Loss Val: 1157893.250 | Loss Avg: 958827.691\n",
      "2024-03-03 13:10:13,265 Epoch:  0 | i: 9 | iter:     0 | LR: 0.0100 | Loss Val: 1124377.750 | Loss Avg: 958076.776\n",
      "2024-03-03 13:10:18,118 Epoch:  0 | i: 10 | iter:     0 | LR: 0.0100 | Loss Val: 1147890.750 | Loss Avg: 958812.839\n",
      "2024-03-03 13:10:22,136 Epoch:  0 | i: 11 | iter:     0 | LR: 0.0100 | Loss Val: 1135507.375 | Loss Avg: 958764.364\n",
      "2024-03-03 13:10:26,484 Epoch:  0 | i: 12 | iter:     0 | LR: 0.0100 | Loss Val: 1111022.125 | Loss Avg: 957162.890\n",
      "2024-03-03 13:10:30,249 Training complete one epoch in 1m 44s\n",
      "2024-03-03 13:11:50,509 Using single source for this experiment.\n",
      "2024-03-03 13:11:50,528 Number of target images:1300\n",
      "2024-03-03 13:11:50,529 Number of source images:1300\n",
      "2024-03-03 13:12:24,991 Epoch:  1 | i: 0 | iter:     0 | LR: 0.0100 | Loss Val: 1153753.125 | Loss Avg: 1153753.125\n",
      "2024-03-03 13:12:30,431 Epoch:  1 | i: 1 | iter:     0 | LR: 0.0100 | Loss Val: 1115222.000 | Loss Avg: 1023700.104\n",
      "2024-03-03 13:12:35,487 Epoch:  1 | i: 2 | iter:     0 | LR: 0.0100 | Loss Val: 1109377.750 | Loss Avg: 995268.137\n",
      "2024-03-03 13:12:40,169 Epoch:  1 | i: 3 | iter:     0 | LR: 0.0100 | Loss Val: 1091443.875 | Loss Avg: 976456.402\n",
      "2024-03-03 13:12:45,424 Epoch:  1 | i: 4 | iter:     0 | LR: 0.0100 | Loss Val: 1123580.125 | Loss Avg: 969538.382\n",
      "2024-03-03 13:12:51,173 Epoch:  1 | i: 5 | iter:     0 | LR: 0.0100 | Loss Val: 1113218.000 | Loss Avg: 963738.074\n",
      "2024-03-03 13:12:55,881 Epoch:  1 | i: 6 | iter:     0 | LR: 0.0100 | Loss Val: 1141069.250 | Loss Avg: 962709.726\n",
      "2024-03-03 13:13:01,061 Epoch:  1 | i: 7 | iter:     0 | LR: 0.0100 | Loss Val: 1149663.875 | Loss Avg: 962568.429\n",
      "2024-03-03 13:13:05,356 Epoch:  1 | i: 8 | iter:     0 | LR: 0.0100 | Loss Val: 1159924.375 | Loss Avg: 963543.665\n",
      "2024-03-03 13:13:10,531 Epoch:  1 | i: 9 | iter:     0 | LR: 0.0100 | Loss Val: 1119385.000 | Loss Avg: 964441.747\n",
      "2024-03-03 13:13:14,705 Epoch:  1 | i: 10 | iter:     0 | LR: 0.0100 | Loss Val: 1093081.750 | Loss Avg: 959942.664\n",
      "2024-03-03 13:13:22,110 Epoch:  1 | i: 11 | iter:     0 | LR: 0.0100 | Loss Val: 1148797.000 | Loss Avg: 960250.981\n",
      "2024-03-03 13:13:27,194 Epoch:  1 | i: 12 | iter:     0 | LR: 0.0100 | Loss Val: 1102106.875 | Loss Avg: 959332.448\n",
      "2024-03-03 13:13:31,563 Training complete one epoch in 1m 41s\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(os.path.dirname(logfile)):\n",
    "        os.makedirs(os.path.dirname(logfile))\n",
    "\n",
    "logging.basicConfig(\n",
    "level=logging.INFO,\n",
    "format=\"%(asctime)s %(message)s\",\n",
    "handlers=[\n",
    "    logging.FileHandler(logfile, \"w\"),\n",
    "    logging.StreamHandler()\n",
    "])\n",
    "\n",
    "logging.info(\"Experiment ID: {}\".format(experimentID))\n",
    "\n",
    "if seed is not None:\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    cudnn.deterministic = True\n",
    "    warnings.warn('You have chosen to seed training. '\n",
    "                    'This will turn on the CUDNN deterministic setting, '\n",
    "                    'which can slow down your training considerably! '\n",
    "                    'You may see unexpected behavior when restarting '\n",
    "                    'from checkpoints.')\n",
    "\n",
    "global best_acc1\n",
    "\n",
    "if device is not None:\n",
    "    logging.info(\"Use GPU: {} for training\".format(device))\n",
    "\n",
    "# create model\n",
    "logging.info(\"=> using pre-trained model '{}'\".format(\"alexnet\"))\n",
    "normalize = NormalizeByChannelMeanStd(\n",
    "mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "model = alexnet(pretrained=True)\n",
    "model.eval()\n",
    "model = nn.Sequential(normalize, model)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # run one epoch\n",
    "    train(model, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
