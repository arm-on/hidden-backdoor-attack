{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "import sys\n",
    "import numpy as np\n",
    "import pdb\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import configparser\n",
    "\n",
    "from PIL import Image\n",
    "from alexnet_fc7out import alexnet, NormalizeByChannelMeanStd\n",
    "from dataset import PoisonGenerationDataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from torch import cuda\n",
    "from torch.backends import mps\n",
    "import glob\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else ('mps' if mps.is_available() else 'cpu')\n",
    "seed = 50\n",
    "experimentID = 'arman'\n",
    "patch_size = 30\n",
    "trigger_id = 10\n",
    "eps = 16\n",
    "logfile = 'logs/report.log'\n",
    "epochs = 2\n",
    "lr = 0.01\n",
    "num_source = 1\n",
    "rand_loc = True\n",
    "target_wnid = 'n02437312'\n",
    "num_iter = 2\n",
    "source_wnid_list = 'source_wnid_list.txt'\n",
    "data_root = '/Users/armanmalekzadeh/Documents/GitHub/hidden-trigger-backdoor-attack/data' # should contain a folder named `train` (containing n0123123 folders) and another folder named `test` with the same structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added by me\n",
    "with open(f'ImageNet_data_list/poison_generation/{target_wnid}.txt', 'w+') as file:\n",
    "    filelist = glob.glob(os.path.join(data_root, 'train', str(target_wnid), '*.JPEG'), recursive=True)\n",
    "    filelist = [f'{target_wnid}/' + os.path.basename(file_path) + ('\\n' if idx!=len(filelist)-1 else '') for idx, file_path in enumerate(filelist)]\n",
    "    file.writelines(filelist)\n",
    "    \n",
    "with open(source_wnid_list, 'r') as file:\n",
    "    all_source_wnids = file.readlines()\n",
    "all_source_wnids = [wnid.strip() for wnid in all_source_wnids]\n",
    "for wnid in all_source_wnids:\n",
    "    with open(f'ImageNet_data_list/poison_generation/{wnid}.txt', 'w+') as file:\n",
    "        filelist = glob.glob(os.path.join(data_root, 'train', str(wnid), '*.JPEG'), recursive=True)\n",
    "        filelist = [f'{wnid}/' + os.path.basename(file_path) + ('\\n' if idx!=len(filelist)-1 else '') for idx, file_path in enumerate(filelist)]\n",
    "        file.writelines(filelist)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveDir_poison = \"poison_data/\" + experimentID + \"/rand_loc_\" +  str(rand_loc) + '/eps_' + str(eps) + \\\n",
    "\t\t\t\t\t'/patch_size_' + str(patch_size) + '/trigger_' + str(trigger_id)\n",
    "saveDir_patched = \"patched_data/\" + experimentID + \"/rand_loc_\" +  str(rand_loc) + '/eps_' + str(eps) + \\\n",
    "\t\t\t\t\t'/patch_size_' + str(patch_size) + '/trigger_' + str(trigger_id)\n",
    "\n",
    "if os.path.exists(saveDir_poison):\n",
    "    shutil.rmtree(saveDir_poison)\n",
    "if os.path.exists(saveDir_patched):\n",
    "    shutil.rmtree(saveDir_patched)\n",
    "\n",
    "if not os.path.exists(saveDir_poison):\n",
    "\tos.makedirs(saveDir_poison)\n",
    "if not os.path.exists(saveDir_patched):\n",
    "\tos.makedirs(saveDir_patched)\n",
    "\n",
    "if not os.path.exists(\"data/{}\".format(experimentID)):\n",
    "\tos.makedirs(\"data/{}\".format(experimentID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(img):\n",
    "\tnpimg = img.numpy()\n",
    "\t# plt.figure()\n",
    "\tplt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(img, fname):\n",
    "\timg = img.data.numpy()\n",
    "\timg = np.transpose(img, (1, 2, 0))\n",
    "\timg = img[: , :, ::-1]\n",
    "\tcv2.imwrite(fname, np.uint8(255 * img), [cv2.IMWRITE_PNG_COMPRESSION, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def adjust_learning_rate(lr, iter):\n",
    "\t\"\"\"Sets the learning rate to the initial LR decayed by 0.5 every 1000 iterations\"\"\"\n",
    "\tlr = lr * (0.5 ** (iter // 1000))\n",
    "\treturn lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "\t\"\"\"Computes and stores the average and current value\"\"\"\n",
    "\tdef __init__(self):\n",
    "\t\tself.reset()\n",
    "\n",
    "\tdef reset(self):\n",
    "\t\tself.val = 0\n",
    "\t\tself.avg = 0\n",
    "\t\tself.sum = 0\n",
    "\t\tself.count = 0\n",
    "\n",
    "\tdef update(self, val, n=1):\n",
    "\t\tself.val = val\n",
    "\t\tself.sum += val * n\n",
    "\t\tself.count += n\n",
    "\t\tself.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch):\n",
    "\n",
    "\tsince = time.time()\n",
    "\t# AVERAGE METER\n",
    "\tlosses = AverageMeter()\n",
    "\n",
    "\t# TRIGGER PARAMETERS\n",
    "\ttrans_image = transforms.Compose([transforms.Resize((224, 224)),\n",
    "\t\t\t\t\t\t\t\t\t  transforms.ToTensor(),\n",
    "\t\t\t\t\t\t\t\t\t  ])\n",
    "\ttrans_trigger = transforms.Compose([transforms.Resize((patch_size, patch_size)),\n",
    "\t\t\t\t\t\t\t\t\t\ttransforms.ToTensor(),\n",
    "\t\t\t\t\t\t\t\t\t\t])\n",
    "\n",
    "\t# PERTURBATION PARAMETERS\n",
    "\teps1 = (eps/255.0)\n",
    "\tlr1 = lr\n",
    "\n",
    "\ttrigger = Image.open('data/triggers/trigger_{}.png'.format(trigger_id)).convert('RGB')\n",
    "\ttrigger = trans_trigger(trigger).unsqueeze(0).to(device)\n",
    "\n",
    "\t# SOURCE AND TARGET DATASETS\n",
    "\ttarget_filelist = \"ImageNet_data_list/poison_generation/\" + target_wnid + \".txt\"\n",
    "\n",
    "\t# Use source wnid list\n",
    "\tif num_source==1:\n",
    "\t\tlogging.info(\"Using single source for this experiment.\")\n",
    "\telse:\n",
    "\t\tlogging.info(\"Using multiple source for this experiment.\")\n",
    "\n",
    "\twith open(\"data/{}/multi_source_filelist.txt\".format(experimentID),\"w\") as f1:\n",
    "\t\twith open(source_wnid_list) as f2:\n",
    "\t\t\tsource_wnids = f2.readlines()\n",
    "\t\t\tsource_wnids = [s.strip() for s in source_wnids]\n",
    "\n",
    "\t\t\tfor source_wnid in source_wnids:\n",
    "\t\t\t\twith open(\"ImageNet_data_list/poison_generation/\" + source_wnid + \".txt\", \"r\") as f2:\n",
    "\t\t\t\t\tshutil.copyfileobj(f2, f1)\n",
    "\n",
    "\tsource_filelist = \"data/{}/multi_source_filelist.txt\".format(experimentID)\n",
    "\n",
    "\n",
    "\tdataset_target = PoisonGenerationDataset(data_root + \"/train\", target_filelist, trans_image)\n",
    "\tdataset_source = PoisonGenerationDataset(data_root + \"/train\", source_filelist, trans_image)\n",
    "\n",
    "\t# SOURCE AND TARGET DATALOADERS\n",
    "\ttrain_loader_target = torch.utils.data.DataLoader(dataset_target,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tbatch_size=100,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tshuffle=True,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tnum_workers=8,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tpin_memory=True)\n",
    "\n",
    "\ttrain_loader_source = torch.utils.data.DataLoader(dataset_source,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t  batch_size=100,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t  shuffle=True,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t  num_workers=8,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t  pin_memory=True)\n",
    "\n",
    "\n",
    "\tlogging.info(\"Number of target images:{}\".format(len(dataset_target)))\n",
    "\tlogging.info(\"Number of source images:{}\".format(len(dataset_source)))\n",
    "\n",
    "\t# USE ITERATORS ON DATALOADERS TO HAVE DISTINCT PAIRING EACH TIME\n",
    "\titer_target = iter(train_loader_target)\n",
    "\titer_source = iter(train_loader_source)\n",
    "\n",
    "\tnum_poisoned = 0\n",
    "\tfor i in range(len(train_loader_target)):\n",
    "\n",
    "\t\t# LOAD ONE BATCH OF SOURCE AND ONE BATCH OF TARGET\n",
    "\t\t(input1, path1) = next(iter_source)\n",
    "\t\t(input2, path2) = next(iter_target)\n",
    "\n",
    "\t\timg_ctr = 0\n",
    "\n",
    "\t\tinput1 = input1.to(device)\n",
    "\t\tinput2 = input2.to(device)\n",
    "\t\tpert = nn.Parameter(torch.zeros_like(input2, requires_grad=True).to(device))\n",
    "\n",
    "\t\tfor z in range(input1.size(0)):\n",
    "\t\t\tif not rand_loc:\n",
    "\t\t\t\tstart_x = 224-patch_size-5\n",
    "\t\t\t\tstart_y = 224-patch_size-5\n",
    "\t\t\telse:\n",
    "\t\t\t\tstart_x = random.randint(0, 224-patch_size-1)\n",
    "\t\t\t\tstart_y = random.randint(0, 224-patch_size-1)\n",
    "\n",
    "\t\t\t# PASTE TRIGGER ON SOURCE IMAGES\n",
    "\t\t\tinput1[z, :, start_y:start_y+patch_size, start_x:start_x+patch_size] = trigger\n",
    "\n",
    "\t\toutput1, feat1 = model(input1)\n",
    "\t\tfeat1 = feat1.detach().clone()\n",
    "\n",
    "\t\tfor k in range(input1.size(0)):\n",
    "\t\t\timg_ctr = img_ctr+1\n",
    "\t\t\t# input2_pert = (pert[k].clone().cpu())\n",
    "\n",
    "\t\t\tfname = saveDir_patched + '/' + 'badnet_' + str(os.path.basename(path1[k])).split('.')[0] + '_' + 'epoch_' + str(epoch).zfill(2)\\\n",
    "\t\t\t\t\t+ str(img_ctr).zfill(5)+'.png'\n",
    "\n",
    "\t\t\tsave_image(input1[k].clone().cpu(), fname)\n",
    "\t\t\tnum_poisoned +=1\n",
    "\n",
    "\t\tfor j in range(num_iter):\n",
    "\t\t\tlr1 = adjust_learning_rate(lr, j)\n",
    "\n",
    "\t\t\toutput2, feat2 = model(input2+pert)\n",
    "\n",
    "\t\t\t# FIND CLOSEST PAIR WITHOUT REPLACEMENT\n",
    "\t\t\tfeat11 = feat1.clone()\n",
    "\t\t\tdist = torch.cdist(feat1, feat2)\n",
    "\t\t\tfor _ in range(feat2.size(0)):\n",
    "\t\t\t\tdist_min_index = (dist == torch.min(dist)).nonzero().squeeze()\n",
    "\t\t\t\tfeat1[dist_min_index[1]] = feat11[dist_min_index[0]]\n",
    "\t\t\t\tdist[dist_min_index[0], dist_min_index[1]] = 1e5\n",
    "\n",
    "\t\t\tloss1 = ((feat1-feat2)**2).sum(dim=1)\n",
    "\t\t\tloss = loss1.sum()\n",
    "\n",
    "\t\t\tlosses.update(loss.item(), input1.size(0))\n",
    "\n",
    "\t\t\tloss.backward()\n",
    "\n",
    "\t\t\tpert = pert- lr1*pert.grad\n",
    "\t\t\tpert = torch.clamp(pert, -eps1, eps1).detach_()\n",
    "\n",
    "\t\t\tpert = pert + input2\n",
    "\n",
    "\t\t\tpert = pert.clamp(0, 1)\n",
    "\n",
    "\t\t\tif j%100 == 0:\n",
    "\t\t\t\tlogging.info(\"Epoch: {:2d} | i: {} | iter: {:5d} | LR: {:2.4f} | Loss Val: {:5.3f} | Loss Avg: {:5.3f}\"\n",
    "\t\t\t\t\t\t\t .format(epoch, i, j, lr1, losses.val, losses.avg))\n",
    "\n",
    "\t\t\tif loss1.max().item() < 10 or j == (num_iter-1):\n",
    "\t\t\t\tfor k in range(input2.size(0)):\n",
    "\t\t\t\t\timg_ctr = img_ctr+1\n",
    "\t\t\t\t\tinput2_pert = (pert[k].clone().cpu())\n",
    "\n",
    "\t\t\t\t\tfname = saveDir_poison + '/' + 'loss_' + str(int(loss1[k].item())).zfill(5) + '_' + 'epoch_' + \\\n",
    "\t\t\t\t\t\t\tstr(epoch).zfill(2) + '_' + str(os.path.basename(path2[k])).split('.')[0] + '_' + \\\n",
    "\t\t\t\t\t\t\tstr(os.path.basename(path1[k])).split('.')[0] + '_kk_' + str(img_ctr).zfill(5)+'.png'\n",
    "\n",
    "\t\t\t\t\tsave_image(input2_pert, fname)\n",
    "\t\t\t\t\tnum_poisoned +=1\n",
    "\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\tpert = pert - input2\n",
    "\t\t\tpert.requires_grad = True\n",
    "\n",
    "\ttime_elapsed = time.time() - since\n",
    "\tlogging.info('Training complete one epoch in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 17:04:19,769 Experiment ID: arman\n",
      "/var/folders/2k/w49p8c6n2s14x29kyn06m2pr0000gn/T/ipykernel_83610/1449311122.py:18: UserWarning: You have chosen to seed training. This will turn on the CUDNN deterministic setting, which can slow down your training considerably! You may see unexpected behavior when restarting from checkpoints.\n",
      "  warnings.warn('You have chosen to seed training. '\n",
      "2024-03-02 17:04:19,789 Use GPU: mps for training\n",
      "2024-03-02 17:04:19,793 => using pre-trained model 'alexnet'\n",
      "2024-03-02 17:04:22,057 Using single source for this experiment.\n",
      "2024-03-02 17:04:22,082 Number of target images:1300\n",
      "2024-03-02 17:04:22,082 Number of source images:1300\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(os.path.dirname(logfile)):\n",
    "        os.makedirs(os.path.dirname(logfile))\n",
    "\n",
    "logging.basicConfig(\n",
    "level=logging.INFO,\n",
    "format=\"%(asctime)s %(message)s\",\n",
    "handlers=[\n",
    "    logging.FileHandler(logfile, \"w\"),\n",
    "    logging.StreamHandler()\n",
    "])\n",
    "\n",
    "logging.info(\"Experiment ID: {}\".format(experimentID))\n",
    "\n",
    "if seed is not None:\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    cudnn.deterministic = True\n",
    "    warnings.warn('You have chosen to seed training. '\n",
    "                    'This will turn on the CUDNN deterministic setting, '\n",
    "                    'which can slow down your training considerably! '\n",
    "                    'You may see unexpected behavior when restarting '\n",
    "                    'from checkpoints.')\n",
    "\n",
    "global best_acc1\n",
    "\n",
    "if device is not None:\n",
    "    logging.info(\"Use GPU: {} for training\".format(device))\n",
    "\n",
    "# create model\n",
    "logging.info(\"=> using pre-trained model '{}'\".format(\"alexnet\"))\n",
    "normalize = NormalizeByChannelMeanStd(\n",
    "mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "model = alexnet(pretrained=True)\n",
    "model.eval()\n",
    "model = nn.Sequential(normalize, model)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # run one epoch\n",
    "    train(model, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
